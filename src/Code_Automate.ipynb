{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc55ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import rasterio as rs\n",
    "import rasterio.features as rsf\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "from pyproj import CRS\n",
    "# from sklearn.preprocessing import Normalize\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ab8a5",
   "metadata": {},
   "source": [
    "Data Preprocessing: \n",
    "## Exporting Mosaicked and Masked Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MainF_str = input(\"Give the location of folder where S2A folders are stored: \")\n",
    "# MainF_str = \"Granule_Data_Banaskantha/\"\n",
    "MainF_str = \"D:/Manav/Granule_Data_Banaskantha_23_24/\"\n",
    "MainF_files = os.listdir(MainF_str)\n",
    "specDs = [\"R10m\"]\n",
    "bands = [\"B02\",\"B03\",\"B04\",\"B08\"]\n",
    "\n",
    "# four sections of granules required for banaskantha district:\n",
    "# BL -> Bottom Left - QYM\n",
    "# BR -> Bottom Right - QZM\n",
    "# UL -> Upper Left - RYN\n",
    "# UR -> Upper Right - RZN\n",
    "\n",
    "for j in tqdm(range(len(specDs))):\n",
    "    specD = specDs[j]\n",
    "    for k in tqdm(range(len(bands))):\n",
    "        band = bands[k]\n",
    "        if os.path.exists('Automate/Mosaicked_Data/'+specD+'_'+band) == False:\n",
    "            os.makedirs('Automate/Mosaicked_Data/'+specD+'_'+band)\n",
    "        \n",
    "        # Step 1: Extracting location of data granules\n",
    "        BL_loc_files = []\n",
    "        BR_loc_files = []\n",
    "        UL_loc_files = []\n",
    "        UR_loc_files = []\n",
    "        for i in range(len(MainF_files)):\n",
    "            fname = MainF_files[i]\n",
    "            if fname[41:44] == \"QYM\":\n",
    "                SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "                SecFolder_File = os.listdir(SecFolder_str)\n",
    "                SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(specD) != -1:\n",
    "                        SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(band) != -1:\n",
    "                        BL_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "            if fname[41:44] == \"QZM\":\n",
    "                SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "                SecFolder_File = os.listdir(SecFolder_str)\n",
    "                SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(specD) != -1:\n",
    "                        SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(band) != -1:\n",
    "                        BR_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "            if fname[41:44] == \"RYN\":\n",
    "                SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "                SecFolder_File = os.listdir(SecFolder_str)\n",
    "                SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(specD) != -1:\n",
    "                        SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(band) != -1:\n",
    "                        UL_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "            if fname[41:44] == \"RZN\":\n",
    "                SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "                SecFolder_File = os.listdir(SecFolder_str)\n",
    "                SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(specD) != -1:\n",
    "                        SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "                SecFolder_Files = os.listdir(SecFolder_str)\n",
    "                for j in range(len(SecFolder_Files)):\n",
    "                    if SecFolder_Files[j].find(band) != -1:\n",
    "                        UR_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "\n",
    "        # Arranging Chronologically\n",
    "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "        dates = [re.search(date_pattern, path).groups() for path in BL_loc_files]\n",
    "        BL_loc_files = [path for _, path in sorted(zip(dates, BL_loc_files))]\n",
    "        \n",
    "        # Extracting Dates\n",
    "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "        dates = [re.search(date_pattern, path).groups() for path in BL_loc_files]\n",
    "        dates_BL = []\n",
    "        for i in range(len(dates)):\n",
    "            tmp = dates[i][0]+dates[i][1]+dates[i][2]\n",
    "            dates_BL.append(tmp)\n",
    "\n",
    "        # Arranging Chronologically\n",
    "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "        dates = [re.search(date_pattern, path).groups() for path in BR_loc_files]\n",
    "        BR_loc_files = [path for _, path in sorted(zip(dates, BR_loc_files))]\n",
    "\n",
    "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "        dates = [re.search(date_pattern, path).groups() for path in UL_loc_files]\n",
    "        UL_loc_files = [path for _, path in sorted(zip(dates, UL_loc_files))]\n",
    "\n",
    "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "        dates = [re.search(date_pattern, path).groups() for path in UR_loc_files]\n",
    "        UR_loc_files = [path for _, path in sorted(zip(dates, UR_loc_files))]\n",
    "\n",
    "\n",
    "        # Step 2: Mosaicking Data Granules\n",
    "        for i in tqdm(range(len(dates_BL)),desc=\"Mosaicking Granules\"):\n",
    "            tmp_img,tmp_transform = merge([BL_loc_files[i],BR_loc_files[i],UL_loc_files[i],UR_loc_files[i]])\n",
    "            print(\"Mosaicking Completed\")\n",
    "            dst_crs = CRS.from_epsg(32642)\n",
    "            with rs.open(\n",
    "                'Automate/Mosaicked_Data/'+specD+'_'+band+'/Merged_'+dates_BL[i]+'_'+specD+'_'+band+'.tif',\n",
    "                'w+',\n",
    "                driver='GTiff',\n",
    "                height=tmp_img.shape[1],\n",
    "                width=tmp_img.shape[2],\n",
    "                count=tmp_img.shape[0],\n",
    "                dtype=np.int32,\n",
    "                crs=dst_crs,\n",
    "                transform=tmp_transform,\n",
    "            ) as dest_file:\n",
    "                dest_file.write(tmp_img)\n",
    "            dest_file.close()\n",
    "            clear_output(wait=True)\n",
    "            print(\"Image Exported\")\n",
    "            print(\"Progress: \",(round((i+1)/(len(dates_BL))*100)))\n",
    "        \n",
    "        # Step 1: Masking Mosaicked Data with District + Agriculture Mask\n",
    "        # Extracting location of Mosaicked Data\n",
    "        MainFolder_str = \"Automate/Mosaicked_Data/\"\n",
    "        MainFolder_files = os.listdir(MainFolder_str)\n",
    "        loc_merged = []\n",
    "        for i in range(len(MainFolder_files)):\n",
    "            MainFolder_files[i] = MainFolder_str+MainFolder_files[i]\n",
    "        for i in range(len(MainFolder_files)):\n",
    "            loc_tmp = []\n",
    "            Subfolder_files = os.listdir(MainFolder_files[i])\n",
    "            for j in range(len(Subfolder_files)):\n",
    "                loc_tmp.append(MainFolder_files[i]+\"/\"+Subfolder_files[j])\n",
    "            loc_merged.append(loc_tmp)\n",
    "        \n",
    "        \n",
    "#         loc_agrimask = input(\"Give the location of agriculture mask for masking process\")\n",
    "#         banasAgriShp = gpd.read_file(loc_agrimask)\n",
    "        banasAgriShp = gpd.read_file(\"Shape Files/Final Shape Files/Banas_agriOnlyMask.shp\")\n",
    "        geometry = []\n",
    "        for i in banasAgriShp['geometry']:\n",
    "            geometry.append(i)\n",
    "        \n",
    "        if os.path.exists('Automate/Masked_Data/'+specD+'_'+band) == False:\n",
    "            os.makedirs('Automate/Masked_Data/'+specD+'_'+band)\n",
    "        \n",
    "        #Masking and Stacking\n",
    "        for i in tqdm(range(len(loc_merged)),desc=\"Main Loop\"):\n",
    "            tmp_stacked = []\n",
    "            for j in tqdm(range(3,len(loc_merged[i])),desc=\"Sub Loop\"):\n",
    "                fname = loc_merged[i][j][-12:-4]\n",
    "                tmp_r = rs.open(loc_merged[i][j],'r')\n",
    "                tmp_data,tmp_transform = mask(dataset = tmp_r, shapes = geometry, crop=True)\n",
    "                tmp_stacked.append(tmp_data[0])\n",
    "                print(\"Masking Completed - \"+str(fname)+\", \"+str(j))\n",
    "                clear_output(wait=True)\n",
    "            tmp_stacked = np.array(tmp_stacked)\n",
    "            dst_crs = CRS.from_epsg(32642)\n",
    "            with rs.open(\n",
    "                'Automate/Masked_Data/'+fname+'/Masked_Stacked_1NOV_15MAR_'+fname+'.tif',\n",
    "                'w',\n",
    "                driver='GTiff',\n",
    "                height=tmp_stacked.shape[1],\n",
    "                width=tmp_stacked.shape[2],\n",
    "                count=tmp_stacked.shape[0],\n",
    "                dtype=np.float32,\n",
    "                crs=dst_crs,\n",
    "                transform=tmp_transform,\n",
    "            ) as dest_file:\n",
    "                dest_file.write(tmp_stacked)\n",
    "            dest_file.close()\n",
    "            print(\"Image Exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a373e3",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "# Extracting Vegetation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae68228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Automate/Masked_Data/R10m_B03/Masked_Stacked_1NOV_15MARR10m_B03.tif'],\n",
       " ['Automate/Masked_Data/R10m_B04/Masked_Stacked_1NOV_15MARR10m_B04.tif'],\n",
       " ['Automate/Masked_Data/R10m_B08/Masked_Stacked_1NOV_15MARR10m_B08.tif']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_band_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8839056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:56<00:00, 78.73s/it]\n",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [02:19<02:19, 139.06s/it]C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_7784\\2848005634.py:51: RuntimeWarning: invalid value encountered in sqrt\n",
      "  arr_vi.append(0.5 * (((2 * arr_band[band_dict[\"B08\"]]) + 1) - (np.sqrt((2 * arr_band[band_dict[\"B08\"]] + 1)**2 + 8 * (arr_band[band_dict[\"B08\"]] - arr_band[band_dict[\"B04\"]])))))\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:32<00:00, 226.35s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [04:17<00:00, 128.52s/it]\n"
     ]
    }
   ],
   "source": [
    "VI = [\"EVI2\",\"GNDVI\",\"MSAVIhyper\",\"MTVI1\",\"NDMI\",\"NDVI\",\"OSAVI\",\"PSRI\",\"SAVI3\",\"VARIgreen\"]\n",
    "specDs = [\"R10m\"]\n",
    "bands = [\"B02\",\"B03\",\"B04\",\"B06\",\"B08\",\"B11\"]\n",
    "loc_band_files = []\n",
    "for i in range(len(bands)):\n",
    "    MainF_str = \"Automate/Masked_Data/\"\n",
    "    MainF_files = os.listdir(MainF_str)\n",
    "    loc_tmp = []\n",
    "    for j in range(len(MainF_files)):\n",
    "        PrimaryFolder_str = MainF_str\n",
    "        if MainF_files[j].find(bands[i]) != -1:\n",
    "            PrimaryFolder_str = PrimaryFolder_str+MainF_files[j]\n",
    "            PrimaryFolder_files = os.listdir(PrimaryFolder_str)\n",
    "            for k in range(len(PrimaryFolder_files)):\n",
    "                loc_tmp.append(PrimaryFolder_str+\"/\"+PrimaryFolder_files[k])\n",
    "    loc_band_files.append(loc_tmp)\n",
    "    \n",
    "# importing images into array\n",
    "arr_band = []\n",
    "for i in tqdm(range(len(loc_band_files))):\n",
    "    arr_tmp = []\n",
    "    for j in range(len(loc_band_files[i])):\n",
    "        r = rs.open(loc_band_files[i][j])\n",
    "        a = r.read()\n",
    "        arr_tmp.append(a)\n",
    "    arr_band.append(arr_tmp[0])\n",
    "arr_band = np.array(arr_band)\n",
    "\n",
    "# changing the range of image arrays\n",
    "arr_band = arr_band/10000\n",
    "\n",
    "# assigning indices to band\n",
    "# Dictionary => band_dict = {\"B02\": 0, \"B03\":1, ....}\n",
    "band_dict = {}\n",
    "for i in range(len(bands)):\n",
    "    band_dict[bands[i]] = i\n",
    "    \n",
    "# assigning numbers to VIs\n",
    "# Dictionary => vi_dict = {\"MTVI1\": 0, \"MSAVIhyper\":1, ....}\n",
    "vi_dict = {}\n",
    "for i in range(len(VI)):\n",
    "    vi_dict[VI[i]] = i\n",
    "\n",
    "# generating mtvi and msavi arrays\n",
    "arr_vi = []\n",
    "for i in tqdm(range(len(VI))):\n",
    "    arr_tmp = []\n",
    "    if VI[i] == \"MTVI1\":\n",
    "        arr_vi.append(1.2*((1.2*(arr_band[band_dict[\"B08\"]]-arr_band[band_dict[\"B03\"]]))-(2.5*(arr_band[band_dict[\"B04\"]]-arr_band[band_dict[\"B03\"]]))))\n",
    "    elif VI[i] == \"MSAVIhyper\":\n",
    "        arr_vi.append(0.5 * (((2 * arr_band[band_dict[\"B08\"]]) + 1) - (np.sqrt((2 * arr_band[band_dict[\"B08\"]] + 1)**2 + 8 * (arr_band[band_dict[\"B08\"]] - arr_band[band_dict[\"B04\"]])))))\n",
    "    elif VI[i] == \"NDVI\":\n",
    "        arr_vi.append((arr_band[band_dict[\"B08\"]] - arr_band[band_dict[\"B04\"]])/(arr_band[band_dict[\"B08\"]] + arr_band[band_dict[\"B04\"]]))\n",
    "    elif VI[i] == \"EVI2\":\n",
    "        arr_vi.append(2.5*((arr_band[band_dict[\"B08\"]]-arr_band[band_dict[\"B04\"]])/(arr_band[band_dict[\"B08\"]]+(6*arr_band[band_dict[\"B04\"]])+1)))\n",
    "    elif VI[i] == \"GNDVI\":\n",
    "        arr_vi.append((arr_band[band_dict[\"B08\"]]-arr_band[band_dict[\"B03\"]])/(arr_band[band_dict[\"B08\"]]+arr_band[band_dict[\"B03\"]]))\n",
    "    elif VI[i] == \"OSAVI\":\n",
    "        arr_vi.append(1.16*(arr_band[band_dict[\"B08\"]]-arr_band[band_dict[\"B04\"]])/(arr_band[band_dict[\"B08\"]]+arr_band[band_dict[\"B04\"]]+0.16))\n",
    "    elif VI[i] == \"PSRI\":\n",
    "        arr_vi.append((arr_band[band_dict[\"B04\"]] - arr_band[band_dict[\"B02\"]])/arr_band[band_dict[\"B06\"]])\n",
    "    elif VI[i] == \"SAVI3\":\n",
    "        arr_vi.append(1.5*(arr_band[band_dict[\"B08\"]] - arr_band[band_dict[\"B04\"]])/(arr_band[band_dict[\"B08\"]] + arr_band[band_dict[\"B04\"]]+0.5))\n",
    "    elif VI[i] == \"VARIgreen\":\n",
    "        arr_vi.append((arr_band[band_dict[\"B03\"]]-arr_band[band_dict[\"B04\"]])/(arr_band[band_dict[\"B03\"]]+arr_band[band_dict[\"B04\"]]+arr_band[band_dict[\"B02\"]]))\n",
    "    elif VI[i] == \"NDMI\":\n",
    "        arr_vi.append((arr_band[band_dict[\"B08\"]] - arr_band[band_dict[\"B11\"]])/(arr_band[band_dict[\"B08\"]] + arr_band[band_dict[\"B11\"]]))\n",
    "arr_vi = np.array(arr_vi,dtype=np.float32)\n",
    "\n",
    "for i in range(len(VI)):\n",
    "    if os.path.exists('Automate/Vegetation_Indices/'+VI[i]+'_R10m/') == False:\n",
    "        os.makedirs('Automate/Vegetation_Indices/'+VI[i]+'_R10m/')\n",
    "\n",
    "r_export = rs.open(loc_band_files[0][0])\n",
    "transform = r_export.profile['transform']            \n",
    "for i in tqdm(range(len(VI))): \n",
    "    dst_crs = CRS.from_epsg(32642)\n",
    "    with rs.open(\n",
    "        'Automate/Vegetation_Indices/'+VI[i]+'_R10m/'+VI[i]+'_Stacked_R10m_1NOV_15MAR.tif',\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=arr_vi[i].shape[1],\n",
    "        width=arr_vi[i].shape[2],\n",
    "        count=arr_vi[i].shape[0],\n",
    "        dtype=np.float32,\n",
    "        crs=dst_crs,\n",
    "        transform=transform,\n",
    "    ) as dest_file:\n",
    "        dest_file.write(arr_vi[i])\n",
    "    dest_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d31ba",
   "metadata": {},
   "source": [
    "## Detecting the range of dates for select bands and VIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fce144",
   "metadata": {},
   "source": [
    "Importing VIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2db102",
   "metadata": {},
   "outputs": [],
   "source": [
    "VI = [\"MTVI1\",\"MSAVIhyper\"]\n",
    "arr_vi = []\n",
    "for i in range(len(VI)):\n",
    "    r_tmp = rs.open('Automate/Vegetation_Indices/'+VI[i]+'_R10m/'+VI[i]+'_Stacked_R10m_1NOV_15MAR.tif')\n",
    "    arr_tmp = r_tmp.read()\n",
    "    arr_vi.append(arr_tmp)\n",
    "arr_vi = np.array(arr_vi,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09741e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give Y pixel coordinate to check the trend of potato time series: 3845\n",
      "Give X pixel coordinate to check the trend of potato time series: 11689\n",
      "[0, 1, 2, 3, 4, 5, 6] 9 6\n"
     ]
    }
   ],
   "source": [
    "potato_pixel = []\n",
    "VI = [\"MTVI1\",\"MSAVIhyper\"]\n",
    "for i in range(2):\n",
    "    if i==1: \n",
    "        tmp = input(\"Give X pixel coordinate to check the trend of potato time series: \")\n",
    "    else:\n",
    "        tmp = input(\"Give Y pixel coordinate to check the trend of potato time series: \")\n",
    "    potato_pixel.append(int(tmp))\n",
    "vals_arr = []\n",
    "vals_arr.append(arr_vi[0][:,potato_pixel[0],potato_pixel[1]])\n",
    "vals_arr.append(arr_vi[1][:,potato_pixel[0],potato_pixel[1]])\n",
    "mtvi_select_diff = [i for i in range(np.argmax(vals_arr[0])+1)]\n",
    "msavi_select_pg = np.argmin(vals_arr[1])\n",
    "mtvi_select_h = np.argmax(vals_arr[0]) + np.argmin(vals_arr[0][(np.argmax(vals_arr[0])):])\n",
    "mtvi_select_h = 9\n",
    "print(mtvi_select_diff,mtvi_select_h,msavi_select_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c8a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:28<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "mtvi_diff = np.zeros((arr_vi.shape[2],arr_vi.shape[3]))\n",
    "for i in tqdm(range(arr_vi.shape[1]-1)):\n",
    "    tmp_arr_t_1 = np.array(arr_vi[0][i+1])\n",
    "    tmp_arr_t = np.array(arr_vi[0][i])\n",
    "    mtvi_diff = mtvi_diff + (abs(tmp_arr_t_1 - tmp_arr_t))**2\n",
    "mtvi_diff = mtvi_diff/arr_vi.shape[1]\n",
    "mtvi_pg = arr_vi[0][mtvi_select_diff[-1]]\n",
    "mtvi_h = arr_vi[0][mtvi_select_h]\n",
    "msavi_pg = arr_vi[1][msavi_select_pg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6bdc8",
   "metadata": {},
   "source": [
    "## Unsupervised Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b400fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4564\\2011044499.py:1: RuntimeWarning: divide by zero encountered in divide\n",
      "  p_feature = (mtvi_diff * mtvi_pg)/(mtvi_h * msavi_pg)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4564\\2011044499.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  p_feature = (mtvi_diff * mtvi_pg)/(mtvi_h * msavi_pg)\n"
     ]
    }
   ],
   "source": [
    "p_feature = (mtvi_diff * mtvi_pg)/(mtvi_h * msavi_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c682fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9877/9877 [07:41<00:00, 21.42it/s]\n"
     ]
    }
   ],
   "source": [
    "threshold = -0.2078\n",
    "segmented_output = np.full(p_feature.shape,np.nan,dtype=np.float32)\n",
    "for i in tqdm(range(p_feature.shape[0])):\n",
    "    for j in range(p_feature.shape[1]):\n",
    "        if np.isnan(p_feature[i,j]) == False:\n",
    "            if p_feature[i,j] < threshold:\n",
    "                segmented_output[i,j] = 1\n",
    "            else:\n",
    "                segmented_output[i,j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10c372",
   "metadata": {},
   "source": [
    "Displaying Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Latitude Longitude\n",
    "lat1, lon1 = 23.84394, 71.20126 \n",
    "lat2, lon2 = 24.27392, 73.02098  \n",
    "x_points = np.linspace(lat1, lat2, segmented_output.shape[1])\n",
    "y_points = np.linspace(lon1, lon2, segmented_output.shape[0])\n",
    "y_points= y_points[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f700936",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Others','Potato','Other Crops']\n",
    "values = np.unique(segmented_output.ravel())\n",
    "values = values[::-1]\n",
    "N = len(labels)\n",
    "clusters = [i+1 for i in range(N)]\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "img = plt.imshow(segmented_output)\n",
    "colors = [img.cmap(img.norm(value)) for value in values]\n",
    "patches = [mpatches.Patch(color=colors[i], label=\"Crop: {0}\".format(labels[i]), edgecolor=\"black\", linewidth=1) for i in range(len(values)) ]\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis='x', labelsize=22)\n",
    "ax.tick_params(axis='y', labelsize=22)\n",
    "\n",
    "x_ticks_interval = 3000\n",
    "y_ticks_interval = 3000\n",
    "x_ticks = np.arange(0, len(x_points), x_ticks_interval)\n",
    "y_ticks = np.arange(0, len(y_points), y_ticks_interval)\n",
    "ax.set_xticks(x_ticks)\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_xticklabels([f'{x_points[i]:.2f}' for i in x_ticks])\n",
    "ax.set_yticklabels([f'{y_points[i]:.4f}' for i in y_ticks])\n",
    "\n",
    "compass_img = plt.imread('compass.png')\n",
    "imagebox = OffsetImage(compass_img, zoom=0.07)\n",
    "ab = AnnotationBbox(imagebox, (0.93, 0.12), xycoords='axes fraction', frameon=False, boxcoords=\"axes fraction\", pad=0)\n",
    "ax.add_artist(ab)\n",
    "\n",
    "t = input(\"Give a title: \")\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0, prop={'size': 'xx-large'})\n",
    "plt.title(t, fontdict={\"size\":27}, pad=30, weight=\"bold\")\n",
    "plt.tight_layout()\n",
    "\n",
    "if os.path.exists('Automate/Outputs/') == False:\n",
    "    os.makedirs('Automate/Outputs/')\n",
    "\n",
    "plt.savefig(\"Automate/Outputs/\"+t+\".png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29a93db",
   "metadata": {},
   "source": [
    "## Supervised Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328e64a",
   "metadata": {},
   "source": [
    "Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66da26fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "import rasterio as rs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from rasterio.plot import show\n",
    "from rasterio.merge import merge\n",
    "import geopandas as gpd\n",
    "from rasterio.features import geometry_mask\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.ops import polygonize\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.enums import Resampling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e9d66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20231101', '20231111', '20231121', '20231201', '20231216', '20231226', '20240110', '20240120', '20240209', '20240219', '20240229', '20240305', '20240315']\n"
     ]
    }
   ],
   "source": [
    "# Extract Dates\n",
    "MainF_str = \"D:/Manav/Granule_Data_Banaskantha_23_24/\"\n",
    "MainF_files = os.listdir(MainF_str)\n",
    "specD = \"R10m\"\n",
    "\n",
    "band = \"B03\"\n",
    "BL_loc_files = []\n",
    "BR_loc_files = []\n",
    "UL_loc_files = []\n",
    "UR_loc_files = []\n",
    "for i in range(len(MainF_files)):\n",
    "    fname = MainF_files[i]\n",
    "    if fname[41:44] == \"QYM\":\n",
    "        SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "        SecFolder_File = os.listdir(SecFolder_str)\n",
    "        SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(specD) != -1:\n",
    "                SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(band) != -1:\n",
    "                BL_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "    if fname[41:44] == \"QZM\":\n",
    "        SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "        SecFolder_File = os.listdir(SecFolder_str)\n",
    "        SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(specD) != -1:\n",
    "                SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(band) != -1:\n",
    "                BR_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "    if fname[41:44] == \"RYN\":\n",
    "        SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "        SecFolder_File = os.listdir(SecFolder_str)\n",
    "        SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(specD) != -1:\n",
    "                SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(band) != -1:\n",
    "                UL_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "    if fname[41:44] == \"RZN\":\n",
    "        SecFolder_str = MainF_str+fname+\"/GRANULE/\"\n",
    "        SecFolder_File = os.listdir(SecFolder_str)\n",
    "        SecFolder_str = SecFolder_str+SecFolder_File[0]+\"/IMG_DATA/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(specD) != -1:\n",
    "                SecFolder_str = SecFolder_str+specD+\"/\"\n",
    "        SecFolder_Files = os.listdir(SecFolder_str)\n",
    "        for j in range(len(SecFolder_Files)):\n",
    "            if SecFolder_Files[j].find(band) != -1:\n",
    "                UR_loc_files.append(SecFolder_str+SecFolder_Files[j])\n",
    "\n",
    "# Arranging Chronologically\n",
    "date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "dates = [re.search(date_pattern, path).groups() for path in BL_loc_files]\n",
    "BL_loc_files = [path for _, path in sorted(zip(dates, BL_loc_files))]\n",
    "\n",
    "# Extracting Dates\n",
    "date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
    "dates = [re.search(date_pattern, path).groups() for path in BL_loc_files]\n",
    "dates_BL = []\n",
    "for i in range(len(dates)):\n",
    "    tmp = dates[i][0]+dates[i][1]+dates[i][2]\n",
    "    dates_BL.append(tmp)\n",
    "dates_BL = dates_BL[3:]\n",
    "print(dates_BL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8314b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                             pixels\n",
      "0  Maize  [[ 5993 15318]\\n [ 5993 15319]\\n [ 5994 15317]...\n",
      "1  Maize  [[ 6088 15129]\\n [ 6089 15129]\\n [ 6090 15129]...\n",
      "2  Maize  [[ 6267 15229]\\n [ 6267 15230]\\n [ 6268 15227]...\n",
      "3  Maize  [[ 6028 15138]\\n [ 6028 15139]\\n [ 6029 15138]...\n",
      "4  Maize  [[ 5987 15136]\\n [ 5987 15137]\\n [ 5988 15136]...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 286/286 [00:22<00:00, 13.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 286/286 [00:22<00:00, 12.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#first gt\n",
    "loc_gt = input(\"Location of Ground truth: \")\n",
    "gt = gpd.read_file(loc_gt)\n",
    "crs_input = input(\"CRS for the GT: \")\n",
    "dst_crs = CRS.from_epsg(crs_input)\n",
    "gt = gt.to_crs(dst_crs)\n",
    "label_colname = input(\"Give the name of crop name column: \")\n",
    "gt_1 = gt[['geometry',label_colname]]\n",
    "geometry = []\n",
    "croplabel = []\n",
    "for i in range(len(gt_1)):  \n",
    "    geometry.append(gt_1['geometry'][i])\n",
    "    croplabel.append(gt_1[label_colname][i])\n",
    "    \n",
    "# second gt\n",
    "loc_gt1 = input(\"Location of Ground Truth: \")\n",
    "gt1 = gpd.read_file(loc_gt1)\n",
    "gt1 = gt1.drop(gt1[gt1['Banas_IS_3']=='Non-Crop-Area'].index)\n",
    "gt1 = gt1.to_crs(dst_crs)\n",
    "label_colname1 = input(\"Give the name of crop name column: \")\n",
    "gt1_1 = gt1[['geometry',label_colname1]]\n",
    "gt1_1 = gt1_1.reset_index()\n",
    "gt1_1 = gt1_1.drop(columns=['index'])\n",
    "\n",
    "geometry1 = []\n",
    "croplabel1 = []\n",
    "for i in range(len(gt1_1)):  \n",
    "    geometry1.append(gt1_1['geometry'][i])\n",
    "    croplabel1.append(gt1_1['Banas_IS_4'][i])\n",
    "    \n",
    "print(\"Ground Truth Processed. Reading Image Data for Training Data Preparation Now.\")\n",
    "r = rs.open(\"Automate/Masked_Data/R10m_B03/Masked_Stacked_1NOV_15MARR10m_B03.tif\")\n",
    "raster_data = np.array(r.read()[0])\n",
    "transform = r.transform\n",
    "coordinates = pd.DataFrame(columns=['label','pixels'])\n",
    "i = 0\n",
    "for index, row in gt_1.iterrows():\n",
    "    polygon = row.geometry\n",
    "    label = row[label_colname]  \n",
    "    \n",
    "    mask = geometry_mask([polygon], out_shape=raster_data.shape, transform=transform, invert=True)\n",
    "    coords = np.argwhere(mask)\n",
    "    \n",
    "    coordinates.loc[len(coordinates.index)] = [label,coords]\n",
    "    print(\"Progress: \",(round(i/len(gt_1)*100,4)))\n",
    "    clear_output(wait=True)\n",
    "    i=i+1\n",
    "    \n",
    "i = 0\n",
    "for index, row in gt1_1.iterrows():\n",
    "    polygon = row.geometry\n",
    "    label = row[label_colname1]\n",
    "    \n",
    "    mask = geometry_mask([polygon], out_shape=raster_data.shape, transform=transform, invert=True)\n",
    "    coords = np.argwhere(mask)\n",
    "    \n",
    "    coordinates.loc[len(coordinates.index)] = [label,coords]\n",
    "    print(\"Progress: \",(round(i/len(gt1_1)*100,4)))\n",
    "    clear_output(wait=True)\n",
    "    i=i+1\n",
    "    \n",
    "if os.path.exists('Automate/Outputs/') == False:\n",
    "    os.makedirs('Automate/Outputs/')\n",
    "    \n",
    "coordinates.to_csv(\"Automate/Outputs/pixels_labels.csv\",index=False)\n",
    "\n",
    "df = pd.read_csv(\"Automate/Outputs/pixels_labels.csv\")\n",
    "print(df.head())\n",
    "df1 = pd.DataFrame(columns=[\"label\",\"pixels\"])\n",
    "\n",
    "# New DataFrame with converted pixel coordinates from string to integer\n",
    "for i in range(len(df)):\n",
    "    output = df['pixels'][i]\n",
    "    numbers = re.findall(r'\\d+', output)\n",
    "    arrays = np.array(numbers).reshape(-1, 2).astype(int)\n",
    "    df1.loc[len(df1.index)] = [df['label'][i],arrays]\n",
    "df1 = df1.drop(len(df1)-1)\n",
    "labels = df1['label'].unique()\n",
    "\n",
    "# Removing Invalid Crops\n",
    "df1 = df1[~df1['label'].isin(['Groundnut','Napier-Grasses','Chikori','Sorghum-Jowar','Pomegranate','Rajgaro','Gram','Tobacco','Cotton'])]\n",
    "\n",
    "# Removing invalid pixels\n",
    "df1 = df1.reset_index()\n",
    "for i in range(len(df1)):\n",
    "    if len(df1['pixels'][i]) == 0:\n",
    "        df1 = df1.drop(i)\n",
    "df1 = df1.reset_index()\n",
    "\n",
    "VI = [\"MTVI1\",\"MSAVIhyper\"]\n",
    "arr_vi = []\n",
    "print(\"Importing Vegetation Indices\")\n",
    "for i in range(len(VI)):\n",
    "    r_tmp = rs.open('Automate/Vegetation_Indices/'+VI[i]+'_R10m/'+VI[i]+'_Stacked_R10m_1NOV_15MAR.tif')\n",
    "    arr_tmp = r_tmp.read()\n",
    "    arr_vi.append(arr_tmp)\n",
    "arr_vi = np.array(arr_vi,dtype=np.float32)\n",
    "\n",
    "print(\"Exporting Training Data\")\n",
    "for k in range(len(VI)):\n",
    "    df_data = pd.DataFrame(columns=['coordinates']+dates_BL+['labels'])\n",
    "    for i in tqdm(range(len(df1))):\n",
    "        coordinates = df1['pixels'][i]\n",
    "        label = df1['label'][i]\n",
    "        for j in coordinates:\n",
    "            values = arr_vi[k][:,j[0],j[1]]\n",
    "            tmp = [j]+list(values)+[label]\n",
    "            df_data.loc[len(df_data.index)] = tmp\n",
    "    df_data.to_csv(\"Automate/Outputs/Training_data_final_\"+VI[k]+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e730c3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Training on a single training data\n",
    "loc_train = input(\"Enter location of training data: \")\n",
    "# df = pd.read_csv(\"Automate/Outputs/Training_data_final_MTVI1.csv\")\n",
    "\n",
    "# extracting VI name\n",
    "mark = -1\n",
    "for i in range(len(loc_train)-1,-1,-1):\n",
    "    if loc_train[i] == \"_\":\n",
    "        mark = i\n",
    "        break\n",
    "VI_name = loc_train[mark+1:-4]\n",
    "\n",
    "df = pd.read_csv(loc_train)\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=[df.columns[0]])\n",
    "label_map = {'Maize':1, 'Wheat':2, 'Mustard':3, 'castor':4, 'Potato':8, 'Musturd':3,\n",
    "       'Castor':4, 'Oat':5, 'Cumin':6, 'Lucerne':7}\n",
    "df['labels_num'] = df['labels'].map(label_map)\n",
    "dates_BL = df.columns[:-2]\n",
    "train_x, test_x, train_y, test_y = train_test_split(df[dates_BL],df['labels_num'],random_state=42,test_size=0.25)\n",
    "\n",
    "choice = input(\"Enter which classifier (Enter Serial Number):\\n1. RF\\n2. XGBoost\\n\")\n",
    "classifier = \"\"\n",
    "if choice == '1':\n",
    "    # Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=25)\n",
    "    classifier = \"RF\"\n",
    "    clf.fit(train_x,train_y)\n",
    "elif choice == '2':\n",
    "    #XGBoost Classifier\n",
    "    clf = XGBClassifier()\n",
    "    classifier = \"XGB\"\n",
    "    clf.fit(train_x,train_y)\n",
    "\n",
    "predict_y = clf.predict(test_x)\n",
    "print(\"Accuracy: {0} %\".format(accuracy_score(test_y,predict_y)*100))\n",
    "print(classification_report(test_y,predict_y))\n",
    "\n",
    "VI = [\"MTVI1\",\"MSAVIhyper\"]\n",
    "arr_vi = []\n",
    "print(\"Importing Vegetation Indices\")\n",
    "for i in range(len(VI)):\n",
    "    r_tmp = rs.open('Automate/Vegetation_Indices/'+VI[i]+'_R10m/'+VI[i]+'_Stacked_R10m_1NOV_15MAR.tif')\n",
    "    arr_tmp = r_tmp.read()\n",
    "    arr_vi.append(arr_tmp)\n",
    "arr_vi = np.array(arr_vi,dtype=np.float32)\n",
    "VI_dict = {}\n",
    "for i in range(len(VI)):\n",
    "    VI_dict[VI[i]] = i\n",
    "\n",
    "print(\"Predicting Using \"+classifier)\n",
    "valid_pixels = np.argwhere(arr_vi[0][0]!=0)\n",
    "values_rows = []\n",
    "def pixel_values(arr):\n",
    "    values = arr_vi[VI_dict[VI_name]][:,arr[0],arr[1]]\n",
    "    tmp = [[arr[0],arr[1]]]+list(values) \n",
    "    return tmp   \n",
    "\n",
    "for i in tqdm(range(len(valid_pixels)), desc=\"Processing\"):\n",
    "    values_rows.append(pixel_values(valid_pixels[i]))\n",
    "    \n",
    "# prediction\n",
    "prediction_y = np.full((arr_vi[VI_dict[VI_name]].shape[1],arr_vi[VI_dict[VI_name]].shape[2]),np.nan,dtype=np.float32)\n",
    "\n",
    "def predict_clf(arr):\n",
    "    return clf.predict(arr)[0]\n",
    "\n",
    "for i in tqdm(range(len(values_rows)), desc=\"Processing\"):\n",
    "    values = values_rows[i][1:]\n",
    "    values = np.reshape(values,(1,-1))\n",
    "    prediction_y[values_rows[i][0][0],values_rows[i][0][1]] = predict_clf(values)\n",
    "    \n",
    "r = rs.open(\"Automate/Masked_Data/R10m_B03/Masked_Stacked_1NOV_15MARR10m_B03.tif\")\n",
    "transform = r.profile['transform']\n",
    "print(transform)\n",
    "\n",
    "if os.path.exists('Automate/Outputs/') == False:\n",
    "    os.makedirs('Automate/Outputs/')\n",
    "\n",
    "dst_crs = CRS.from_epsg(32642)\n",
    "with rs.open(\n",
    "    'Automate/Outputs/Segmented_'+VI+'_'+classifier+'.tif',\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=prediction_y.shape[0],\n",
    "    width=prediction_y.shape[1],\n",
    "    count=1,\n",
    "    dtype=np.int16,\n",
    "    crs=dst_crs,\n",
    "    transform=transform,\n",
    ") as dest_file:\n",
    "    dest_file.write(prediction_y,1)\n",
    "dest_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
